{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install tensorflow","metadata":{"id":"dTluN2ry77iF","outputId":"6f59bee8-81ce-406c-bb57-26fec92a7021","execution":{"iopub.status.busy":"2023-12-26T13:34:40.891697Z","iopub.execute_input":"2023-12-26T13:34:40.892049Z","iopub.status.idle":"2023-12-26T13:34:54.724425Z","shell.execute_reply.started":"2023-12-26T13:34:40.892020Z","shell.execute_reply":"2023-12-26T13:34:54.722892Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.3)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (68.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.32.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.11.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:36:04.607272Z","iopub.execute_input":"2023-12-26T13:36:04.608039Z","iopub.status.idle":"2023-12-26T13:36:04.944361Z","shell.execute_reply.started":"2023-12-26T13:36:04.608013Z","shell.execute_reply":"2023-12-26T13:36:04.943511Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:36:04.946152Z","iopub.execute_input":"2023-12-26T13:36:04.946592Z","iopub.status.idle":"2023-12-26T13:36:04.972242Z","shell.execute_reply.started":"2023-12-26T13:36:04.946565Z","shell.execute_reply":"2023-12-26T13:36:04.971438Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['violence video cleaned', 'Non-Violence Videos', 'Weapon Violence']"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['filename'] = os.listdir(r'/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/Non-Violence Videos')\ndf['path'] = '/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/Non-Violence Videos/'\ndf['category'] = 0\ndf2 = pd.DataFrame()\ndf2['filename'] = os.listdir(r'/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/violence video cleaned')\ndf2['path'] = '/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/violence video cleaned/'\ndf2['category'] = 1\ndf3 = pd.DataFrame()\ndf3['filename'] = os.listdir(r'/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/Weapon Violence')\ndf3['path'] = '/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/Weapon Violence/'\ndf3['category'] = 2\n\ndf = pd.concat([df, df2, df3])\ndf = df.sample(frac = 1)\ndf.head()\ndf.to_csv(\"fights2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:36:04.973291Z","iopub.execute_input":"2023-12-26T13:36:04.973637Z","iopub.status.idle":"2023-12-26T13:36:05.193888Z","shell.execute_reply.started":"2023-12-26T13:36:04.973605Z","shell.execute_reply":"2023-12-26T13:36:05.193024Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:36:05.195929Z","iopub.execute_input":"2023-12-26T13:36:05.196211Z","iopub.status.idle":"2023-12-26T13:36:05.208627Z","shell.execute_reply.started":"2023-12-26T13:36:05.196187Z","shell.execute_reply":"2023-12-26T13:36:05.207683Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     filename                                               path  category\n217  nv69.mp4  /kaggle/input/smartcity-cctv-violence-detectio...         0\n115  WV94.mp4  /kaggle/input/smartcity-cctv-violence-detectio...         2\n206  nv87.mp4  /kaggle/input/smartcity-cctv-violence-detectio...         0\n84   WV36.mp4  /kaggle/input/smartcity-cctv-violence-detectio...         2\n46   WV12.mp4  /kaggle/input/smartcity-cctv-violence-detectio...         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>path</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>217</th>\n      <td>nv69.mp4</td>\n      <td>/kaggle/input/smartcity-cctv-violence-detectio...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>WV94.mp4</td>\n      <td>/kaggle/input/smartcity-cctv-violence-detectio...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>nv87.mp4</td>\n      <td>/kaggle/input/smartcity-cctv-violence-detectio...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>WV36.mp4</td>\n      <td>/kaggle/input/smartcity-cctv-violence-detectio...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>WV12.mp4</td>\n      <td>/kaggle/input/smartcity-cctv-violence-detectio...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Reshape, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow_datasets as tfds\nimport joblib","metadata":{"id":"fcy2xz7D4rJB","execution":{"iopub.status.busy":"2023-12-26T13:36:05.210057Z","iopub.execute_input":"2023-12-26T13:36:05.210317Z","iopub.status.idle":"2023-12-26T13:36:15.905667Z","shell.execute_reply.started":"2023-12-26T13:36:05.210295Z","shell.execute_reply":"2023-12-26T13:36:15.904650Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"csv_file_path = 'fights2.csv'\ndata = pd.read_csv(csv_file_path)\ndata = data.drop(data[data['filename'] == 'V5.mp4'].index)\ndata.shape\n","metadata":{"id":"NWt9kNZt7zfk","execution":{"iopub.status.busy":"2023-12-26T13:36:15.907075Z","iopub.execute_input":"2023-12-26T13:36:15.907634Z","iopub.status.idle":"2023-12-26T13:36:15.925020Z","shell.execute_reply.started":"2023-12-26T13:36:15.907598Z","shell.execute_reply":"2023-12-26T13:36:15.923946Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(481, 3)"},"metadata":{}}]},{"cell_type":"code","source":"FRAME_WIDTH = 128\nFRAME_HEIGHT = 128\nNUM_CLASSES = 3\nNUM_FRAMES = 30\nBATCH_SIZE = 10","metadata":{"id":"XV2R-W5P9msT","execution":{"iopub.status.busy":"2023-12-26T13:36:15.927058Z","iopub.execute_input":"2023-12-26T13:36:15.927467Z","iopub.status.idle":"2023-12-26T13:36:15.937816Z","shell.execute_reply.started":"2023-12-26T13:36:15.927430Z","shell.execute_reply":"2023-12-26T13:36:15.936949Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def load_and_preprocess_video(file_path):\n    cap = cv2.VideoCapture(file_path)\n    frames = []\n    # NUM_FRAMES = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    for _ in range(NUM_FRAMES):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n        frames.append(frame)\n\n    cap.release()\n\n    # Pad or truncate frames to ensure a fixed number\n    while len(frames) < NUM_FRAMES:\n        frames.append(np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8))\n\n    return np.array(frames)","metadata":{"id":"1eNKW6yB8JuX","execution":{"iopub.status.busy":"2023-12-26T13:36:15.939443Z","iopub.execute_input":"2023-12-26T13:36:15.939788Z","iopub.status.idle":"2023-12-26T13:36:15.949369Z","shell.execute_reply.started":"2023-12-26T13:36:15.939757Z","shell.execute_reply":"2023-12-26T13:36:15.948531Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n\n# Define a custom data generator function\ndef data_generator(data, batch_size):\n    num_samples = data.shape[0]\n    while True:\n        indices = np.arange(num_samples)\n        np.random.shuffle(indices)\n        for start in range(0, num_samples, batch_size):\n            end = min(start + batch_size, num_samples)\n            batch_indices = indices[start:end]\n            batch_data = data.iloc[batch_indices]\n            batch_data.to_csv('bdata.csv')\n            batch_X = []\n            batch_y = []\n            \n            for _, row in batch_data.iterrows():\n                video_file = row['filename']\n                path  = row['path']\n                path += video_file\n                label = row['category']\n                frames = load_and_preprocess_video(path)\n                frames = list(map(lambda x :x/255, frames))\n\n                batch_X.append(frames)\n                batch_y.append(label)\n                del frames\n            \n            batch_X = np.array(batch_X)\n            batch_y = np.array(batch_y)\n            yield batch_X, batch_y","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:36:15.950789Z","iopub.execute_input":"2023-12-26T13:36:15.951228Z","iopub.status.idle":"2023-12-26T13:36:15.964998Z","shell.execute_reply.started":"2023-12-26T13:36:15.951197Z","shell.execute_reply":"2023-12-26T13:36:15.964064Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# # Build the 3D CNN model\n# model = Sequential()\n# model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(NUM_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3)))\n# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n# model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))\n# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n# model.add(Conv3D(128, (3, 3, 3), activation='relu'))\n# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n# shape = model.layers[-1].output_shape\n# model.add(Reshape((shape[-1],shape[1]*shape[2]*shape[3])))\n# model.add(LSTM(64, return_sequences=True))\n# model.add(LSTM(64))\n# model.add(Dropout(.5))\n\n# model.add((Flatten()))\n\n# # # FC layers group\n\n# model.add(Dense(64, activation='relu'))\n# model.add(Dropout(.5))\n# model.add(Dense(NUM_CLASSES, activation='softmax'))\n\n# # Compile the model\n# model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# # Train the model\n# # model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n# train_data_generator = data_generator(data_train, BATCH_SIZE)\n# validation_data = data_generator(data_test, BATCH_SIZE)\n# history = model.fit(\n#     train_data_generator,\n#     steps_per_epoch=len(data_train) // BATCH_SIZE,\n#     epochs=10,\n#     validation_data=validation_data,\n#     validation_steps=len(data_test) // BATCH_SIZE\n# )\n# test_loss, test_accuracy = model.evaluate(validation_data, steps=len(data_test) // BATCH_SIZE)\n# print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n# file1 = open('accuracy.txt', 'w')\n# file1.write(f'Test accuracy: {test_accuracy * 100:.2f}%')\n# file1.close()\n\n# model.save('fights2vgg.keras')\n# f.close()\n","metadata":{"id":"jQA-pwV3BImC","outputId":"15a715b7-0bd7-4ae9-c156-4504b7be41bb","execution":{"iopub.status.busy":"2023-10-02T07:10:11.310933Z","iopub.execute_input":"2023-10-02T07:10:11.311851Z","iopub.status.idle":"2023-10-02T07:10:11.320585Z","shell.execute_reply.started":"2023-10-02T07:10:11.311822Z","shell.execute_reply":"2023-10-02T07:10:11.319559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\n# Define a directory to save the model checkpoints\ncheckpoint_dir = 'model_checkpoints/'\n\n# Ensure the directory exists\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Create a ModelCheckpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    filepath=os.path.join(checkpoint_dir, 'model_weights_vgg.h5'),  # Save with epoch number\n    save_best_only=False,  # Save model weights after each epoch\n    save_weights_only=True,  # Save only the weights, not the entire model\n    verbose=1  # Display progress during saving\n)\n\nearly_stopping = EarlyStopping(patience = 2, monitor='val_loss',\n                                 mode='min', restore_best_weights=True, \n                                 verbose = 1, min_delta = .00075)\nclass myCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if ((logs.get('accuracy')>=0.999)):\n            print(\"\\nLimits Reached cancelling training!\")\n            self.model.stop_training = True\nend_callback = myCallback()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-07T10:46:13.620352Z","iopub.execute_input":"2023-10-07T10:46:13.620688Z","iopub.status.idle":"2023-10-07T10:46:13.627270Z","shell.execute_reply.started":"2023-10-07T10:46:13.620661Z","shell.execute_reply":"2023-10-07T10:46:13.626340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import TimeDistributed, LSTM, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Flatten, Reshape\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(FRAME_HEIGHT, FRAME_WIDTH, 3))\n\n# Freeze the pre-trained layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Create a custom video classification model\nmodel = Sequential([\n    TimeDistributed(base_model, input_shape=(NUM_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3)),\n    Flatten(),\n    Reshape((NUM_FRAMES, -1)),\n    LSTM(64, return_sequences=True),\n    LSTM(64),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(NUM_CLASSES, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Create the data generator instances\ntrain_data_generator = data_generator(data_train, BATCH_SIZE)\nvalidation_data = data_generator(data_test, BATCH_SIZE)\n# Train the model using fit_generator\nhistory = model.fit(\n    train_data_generator,\n    steps_per_epoch=len(data_train) // BATCH_SIZE,\n    epochs=20,\n    validation_data=validation_data,\n    validation_steps=len(data_test) // BATCH_SIZE,\n    callbacks=[checkpoint_callback, end_callback] \n)\ntest_loss, test_accuracy = model.evaluate(validation_data, steps=len(data_test) // BATCH_SIZE)\nprint(f'Test accuracy: {test_accuracy * 100:.2f}%')\nfile1 = open('accuracy.txt', 'w')\nfile1.write(f'Test accuracy: {test_accuracy * 100:.2f}%')\nfile1.close()\nmodel.save('fights2vgg.keras')","metadata":{"execution":{"iopub.status.busy":"2023-10-07T10:46:14.559866Z","iopub.execute_input":"2023-10-07T10:46:14.560634Z","iopub.status.idle":"2023-10-07T11:12:56.192877Z","shell.execute_reply.started":"2023-10-07T10:46:14.560593Z","shell.execute_reply":"2023-10-07T11:12:56.191567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nmodel = keras.models.load_model('fights2vgg2.keras')\n\ntrain_data_generator = data_generator(data_train, BATCH_SIZE)\nvalidation_data = data_generator(data_test, BATCH_SIZE)\n\n\n\ntest_loss, test_accuracy = model.evaluate(validation_data, steps=len(data_test) // BATCH_SIZE)\nprint(f'Test accuracy: {test_accuracy * 100:.2f}%')\nfile1 = open('accuracy.txt', 'w')\nfile1.write(f'Test accuracy: {test_accuracy * 100:.2f}%')\nfile1.close()\nmodel.save('fights2vgg2.keras')","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:36:42.188055Z","iopub.execute_input":"2023-10-07T11:36:42.189032Z","iopub.status.idle":"2023-10-07T11:37:09.936579Z","shell.execute_reply.started":"2023-10-07T11:36:42.188975Z","shell.execute_reply":"2023-10-07T11:37:09.935112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nmodel = keras.models.load_model('fights2vgg2.keras')\nframes = load_and_preprocess_video(r'/kaggle/input/cctv-fights-dataset/dataset/CCTV_DATA/training/fight_0012.mpeg')\nframes = list(map(lambda x :x/255, frames))\nframes = np.expand_dims(frames, axis=0)\np = model.predict(frames)\nprint(p)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:53:21.300357Z","iopub.execute_input":"2023-10-07T11:53:21.300736Z","iopub.status.idle":"2023-10-07T11:53:24.055337Z","shell.execute_reply.started":"2023-10-07T11:53:21.300708Z","shell.execute_reply":"2023-10-07T11:53:24.054094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Only 2 categories","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['filename'] = os.listdir(r'/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/Non-Violence Videos')\ndf['path'] = '/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/Non-Violence Videos/'\ndf['category'] = 0\ndf2 = pd.DataFrame()\ndf2['filename'] = os.listdir(r'/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/violence video cleaned')\ndf2['path'] = '/kaggle/input/smartcity-cctv-violence-detection-dataset-scvd/SCVD/videos/violence video cleaned/'\ndf2['category'] = 1\ndf3 = pd.DataFrame()\ndf3['filename'] = os.listdir(r'/kaggle/input/cctv-fights-dataset/dataset/CCTV_DATA/training')\ndf3['path'] = '/kaggle/input/cctv-fights-dataset/dataset/CCTV_DATA/training/'\ndf3['category'] = 1\n\ndata = pd.concat([df2,df, df3])","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:36:15.967591Z","iopub.execute_input":"2023-12-26T13:36:15.967890Z","iopub.status.idle":"2023-12-26T13:36:16.005618Z","shell.execute_reply.started":"2023-12-26T13:36:15.967867Z","shell.execute_reply":"2023-12-26T13:36:16.004698Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for idx, row in data.iterrows():\n    video_file = row['filename']\n    path = row['path']+video_file\n    label = row['category']\n    frames = load_and_preprocess_video(path)\n    frames = list(map(lambda x :x/255, frames))\n    if np.sum(frames) == 0:\n        data = data.drop(idx)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:36:16.007057Z","iopub.execute_input":"2023-12-26T13:36:16.007370Z","iopub.status.idle":"2023-12-26T13:37:01.687949Z","shell.execute_reply.started":"2023-12-26T13:36:16.007340Z","shell.execute_reply":"2023-12-26T13:37:01.687096Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"[av1 @ 0x5729f14b8840] Your platform doesn't suppport hardware accelerated AV1 decoding.\n[av1 @ 0x5729f14b8840] Failed to get pixel format.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b8840] Missing Sequence Header.\n[av1 @ 0x5729f14b11c0] Your platform doesn't suppport hardware accelerated AV1 decoding.\n[av1 @ 0x5729f14b11c0] Failed to get pixel format.\n","output_type":"stream"}]},{"cell_type":"code","source":"NUM_CLASSES = 2","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:37:11.057207Z","iopub.execute_input":"2023-12-26T13:37:11.057962Z","iopub.status.idle":"2023-12-26T13:37:11.061944Z","shell.execute_reply.started":"2023-12-26T13:37:11.057928Z","shell.execute_reply":"2023-12-26T13:37:11.060981Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def load_and_preprocess_video(file_path):\n    cap = cv2.VideoCapture(file_path)\n    frames = []\n    # NUM_FRAMES = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    for _ in range(NUM_FRAMES):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n        frames.append(frame)\n\n    cap.release()\n\n    # Pad or truncate frames to ensure a fixed number\n    while len(frames) < NUM_FRAMES:\n        frames.append(np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8))\n\n    return np.array(frames)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:37:12.255640Z","iopub.execute_input":"2023-12-26T13:37:12.256633Z","iopub.status.idle":"2023-12-26T13:37:12.263432Z","shell.execute_reply.started":"2023-12-26T13:37:12.256587Z","shell.execute_reply":"2023-12-26T13:37:12.262373Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n\n# Define a custom data generator function\ndef data_generator(data, batch_size):\n    num_samples = data.shape[0]\n    while True:\n        indices = np.arange(num_samples)\n        np.random.shuffle(indices)\n        for start in range(0, num_samples, batch_size):\n            end = min(start + batch_size, num_samples)\n            batch_indices = indices[start:end]\n            batch_data = data.iloc[batch_indices]\n            batch_data.to_csv('bdata.csv')\n            batch_X = []\n            batch_y = []\n            \n            for _, row in batch_data.iterrows():\n                video_file = row['filename']\n                path  = row['path']\n                path += video_file\n                label = row['category']\n                frames = load_and_preprocess_video(path)\n                frames = list(map(lambda x :x/255, frames))\n\n                batch_X.append(frames)\n                batch_y.append(label)\n                del frames\n            \n            batch_X = np.array(batch_X)\n            batch_y = np.array(batch_y)\n            yield batch_X, batch_y","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:37:13.695471Z","iopub.execute_input":"2023-12-26T13:37:13.695827Z","iopub.status.idle":"2023-12-26T13:37:13.706451Z","shell.execute_reply.started":"2023-12-26T13:37:13.695802Z","shell.execute_reply":"2023-12-26T13:37:13.705447Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\n# Define a directory to save the model checkpoints\ncheckpoint_dir = 'model_checkpoints/'\n\n# Ensure the directory exists\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Create a ModelCheckpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    filepath=os.path.join(checkpoint_dir, 'model_weights_vgg_2cat.h5'),  # Save with epoch number\n    save_best_only=False,  # Save model weights after each epoch\n    save_weights_only=True,  # Save only the weights, not the entire model\n    verbose=1  # Display progress during saving\n)\n\nearly_stopping = EarlyStopping(patience = 2, monitor='val_loss',\n                                 mode='min', restore_best_weights=True, \n                                 verbose = 1, min_delta = .000075)\nclass myCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if ((logs.get('accuracy')>=0.999)):\n            print(\"\\nLimits Reached cancelling training!\")\n            self.model.stop_training = True\nend_callback = myCallback()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:37:19.158927Z","iopub.execute_input":"2023-12-26T13:37:19.159301Z","iopub.status.idle":"2023-12-26T13:37:19.167761Z","shell.execute_reply.started":"2023-12-26T13:37:19.159271Z","shell.execute_reply":"2023-12-26T13:37:19.166760Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import TimeDistributed, LSTM, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.layers import Flatten, Reshape\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(FRAME_HEIGHT, FRAME_WIDTH, 3))\n\n# Freeze the pre-trained layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Create a custom video classification model\nmodel = Sequential([\n    TimeDistributed(base_model, input_shape=(NUM_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3)),\n    Flatten(),\n    Reshape((NUM_FRAMES, -1)),\n    LSTM(30, return_sequences=True),\n    LSTM(30),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(NUM_CLASSES, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Create the data generator instances\ntrain_data_generator = data_generator(data_train, BATCH_SIZE)\nvalidation_data = data_generator(data_test, BATCH_SIZE)\n# Train the model using fit_generator\nhistory = model.fit(\n    train_data_generator,\n    steps_per_epoch=len(data_train) // BATCH_SIZE,\n    epochs=20,\n    validation_data=validation_data,\n    validation_steps=len(data_test) // BATCH_SIZE,\n    callbacks=[checkpoint_callback, end_callback] \n)\ntest_loss, test_accuracy = model.evaluate(validation_data, steps=len(data_test) // BATCH_SIZE)\nprint(f'Test accuracy: {test_accuracy * 100:.2f}%')\nfile1 = open('accuracy.txt', 'w')\nfile1.write(f'Test accuracy: {test_accuracy * 100:.2f}%')\nfile1.close()\nmodel.save('fights2vgg2cat2.keras')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:37:56.804826Z","iopub.execute_input":"2023-12-26T13:37:56.805187Z","iopub.status.idle":"2023-12-26T13:52:34.823748Z","shell.execute_reply.started":"2023-12-26T13:37:56.805159Z","shell.execute_reply":"2023-12-26T13:52:34.822599Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\nEpoch 1/20\n39/39 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.6282\nEpoch 1: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 61s 1s/step - loss: 0.6136 - accuracy: 0.6282 - val_loss: 0.5867 - val_accuracy: 0.6889\nEpoch 2/20\n39/39 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.8068\nEpoch 2: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 43s 1s/step - loss: 0.4545 - accuracy: 0.8068 - val_loss: 0.5324 - val_accuracy: 0.7667\nEpoch 3/20\n39/39 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.8590\nEpoch 3: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 42s 1s/step - loss: 0.3486 - accuracy: 0.8590 - val_loss: 0.3834 - val_accuracy: 0.8556\nEpoch 4/20\n39/39 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9191\nEpoch 4: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 43s 1s/step - loss: 0.2277 - accuracy: 0.9191 - val_loss: 0.3984 - val_accuracy: 0.8556\nEpoch 5/20\n39/39 [==============================] - ETA: 0s - loss: 0.2318 - accuracy: 0.9191\nEpoch 5: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 41s 1s/step - loss: 0.2318 - accuracy: 0.9191 - val_loss: 0.5249 - val_accuracy: 0.8000\nEpoch 6/20\n39/39 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9269\nEpoch 6: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 44s 1s/step - loss: 0.2102 - accuracy: 0.9269 - val_loss: 0.3724 - val_accuracy: 0.8556\nEpoch 7/20\n39/39 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9347\nEpoch 7: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 41s 1s/step - loss: 0.1833 - accuracy: 0.9347 - val_loss: 0.5905 - val_accuracy: 0.8222\nEpoch 8/20\n39/39 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9556\nEpoch 8: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 42s 1s/step - loss: 0.1263 - accuracy: 0.9556 - val_loss: 0.5618 - val_accuracy: 0.7889\nEpoch 9/20\n39/39 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9452\nEpoch 9: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 44s 1s/step - loss: 0.1520 - accuracy: 0.9452 - val_loss: 0.4938 - val_accuracy: 0.8556\nEpoch 10/20\n39/39 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9347\nEpoch 10: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 40s 1s/step - loss: 0.1622 - accuracy: 0.9347 - val_loss: 0.4660 - val_accuracy: 0.8444\nEpoch 11/20\n39/39 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9556\nEpoch 11: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 42s 1s/step - loss: 0.1549 - accuracy: 0.9556 - val_loss: 0.4495 - val_accuracy: 0.8444\nEpoch 12/20\n39/39 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9556\nEpoch 12: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 41s 1s/step - loss: 0.1140 - accuracy: 0.9556 - val_loss: 0.4596 - val_accuracy: 0.8778\nEpoch 13/20\n39/39 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9347\nEpoch 13: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 40s 1s/step - loss: 0.1508 - accuracy: 0.9347 - val_loss: 0.6645 - val_accuracy: 0.8333\nEpoch 14/20\n39/39 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9452\nEpoch 14: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 43s 1s/step - loss: 0.1070 - accuracy: 0.9452 - val_loss: 0.5561 - val_accuracy: 0.8556\nEpoch 15/20\n39/39 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9608\nEpoch 15: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 41s 1s/step - loss: 0.0769 - accuracy: 0.9608 - val_loss: 0.7950 - val_accuracy: 0.8222\nEpoch 16/20\n39/39 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9634\nEpoch 16: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 41s 1s/step - loss: 0.0881 - accuracy: 0.9634 - val_loss: 0.6472 - val_accuracy: 0.8444\nEpoch 17/20\n39/39 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9634\nEpoch 17: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 42s 1s/step - loss: 0.0729 - accuracy: 0.9634 - val_loss: 0.8897 - val_accuracy: 0.8333\nEpoch 18/20\n39/39 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9687\nEpoch 18: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 42s 1s/step - loss: 0.0569 - accuracy: 0.9687 - val_loss: 0.8711 - val_accuracy: 0.8333\nEpoch 19/20\n39/39 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9608\nEpoch 19: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 40s 1s/step - loss: 0.0623 - accuracy: 0.9608 - val_loss: 0.8843 - val_accuracy: 0.8333\nEpoch 20/20\n39/39 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9634\nEpoch 20: saving model to model_checkpoints/model_weights_vgg_2cat.h5\n39/39 [==============================] - 39s 1s/step - loss: 0.0569 - accuracy: 0.9634 - val_loss: 0.8447 - val_accuracy: 0.8444\n9/9 [==============================] - 6s 728ms/step - loss: 0.7626 - accuracy: 0.8556\nTest accuracy: 85.56%\n","output_type":"stream"}]},{"cell_type":"code","source":"import keras\nmodel = keras.models.load_model('fights2vgg2cat2.keras')\nframes = load_and_preprocess_video(r'/kaggle/input/cctv-fights-dataset/dataset/CCTV_DATA/testing/fight_0015.mpeg')\nframes = list(map(lambda x :x/255, frames))\nframes = np.expand_dims(frames, axis=0)\np = model.predict(frames)\nprint(p)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T13:54:46.569632Z","iopub.execute_input":"2023-12-26T13:54:46.570009Z","iopub.status.idle":"2023-12-26T13:54:48.799519Z","shell.execute_reply.started":"2023-12-26T13:54:46.569980Z","shell.execute_reply":"2023-12-26T13:54:48.798469Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 809ms/step\n[[0.00138326 0.9986167 ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}