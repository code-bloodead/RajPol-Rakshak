{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from utils.yolo import parse_yolo_predictions\n",
    "# from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climber YOLO Loaded\n"
     ]
    }
   ],
   "source": [
    "climberYoloModel = YOLO(\"weights/climb_walk/climber.pt\")\n",
    "print(\"Climber YOLO Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 1216, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 192x256 1 walker, 38.1ms\n",
      "Speed: 2.4ms preprocess, 38.1ms inference, 0.8ms postprocess per image at shape (1, 3, 192, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'confidence': 0.9060655832290649,\n",
       "  'label': 'walker',\n",
       "  'bbox': [0.5109090805053711,\n",
       "   0.5737704634666443,\n",
       "   0.12727272510528564,\n",
       "   0.5573770403862],\n",
       "  'bbox_std': [140.5, 105.0, 35.0, 102.0],\n",
       "  'orig_shape': (183, 275)}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = cv2.imread(\"../samples/walker2.jpg\")\n",
    "outputs = climberYoloModel.predict(source=frame)\n",
    "parse_yolo_predictions(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 192x256 4 climbers, 35.4ms\n",
      "Speed: 0.9ms preprocess, 35.4ms inference, 0.5ms postprocess per image at shape (1, 3, 192, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'confidence': 0.8434908986091614,\n",
       "  'label': 'climber',\n",
       "  'bbox': [0.3830188810825348,\n",
       "   0.3842105269432068,\n",
       "   0.2528301775455475,\n",
       "   0.5894736647605896],\n",
       "  'bbox_std': [101.5, 73.0, 67.0, 112.0],\n",
       "  'orig_shape': (190, 265)},\n",
       " {'confidence': 0.7502453327178955,\n",
       "  'label': 'climber',\n",
       "  'bbox': [0.9018868207931519,\n",
       "   0.41842105984687805,\n",
       "   0.12075471878051758,\n",
       "   0.4368421137332916],\n",
       "  'bbox_std': [239.0, 79.5, 32.0, 83.0],\n",
       "  'orig_shape': (190, 265)},\n",
       " {'confidence': 0.7155665755271912,\n",
       "  'label': 'climber',\n",
       "  'bbox': [0.6943396329879761,\n",
       "   0.49473685026168823,\n",
       "   0.2339622676372528,\n",
       "   0.621052622795105],\n",
       "  'bbox_std': [184.0, 94.0, 62.0, 118.0],\n",
       "  'orig_shape': (190, 265)},\n",
       " {'confidence': 0.2657436430454254,\n",
       "  'label': 'climber',\n",
       "  'bbox': [0.4433962404727936,\n",
       "   0.4684210419654846,\n",
       "   0.1320754736661911,\n",
       "   0.378947377204895],\n",
       "  'bbox_std': [117.5, 89.0, 35.0, 72.0],\n",
       "  'orig_shape': (190, 265)}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = cv2.imread(\"../samples/climber2.jpg\")\n",
    "outputs = climberYoloModel.predict(source=frame)\n",
    "parse_yolo_predictions(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'climber', 1: 'walker'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 17, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].keypoints.xyn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "boxes: tensor([[211.0000,  78.0000, 239.0000, 118.0000,   0.3894,   1.0000],\n",
       "        [ 27.0000, 107.0000,  54.0000, 155.0000,   0.3645,   1.0000]])\n",
       "cls: tensor([1., 1.])\n",
       "conf: tensor([0.3894, 0.3645])\n",
       "data: tensor([[211.0000,  78.0000, 239.0000, 118.0000,   0.3894,   1.0000],\n",
       "        [ 27.0000, 107.0000,  54.0000, 155.0000,   0.3645,   1.0000]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (183, 275)\n",
       "shape: torch.Size([2, 6])\n",
       "xywh: tensor([[225.0000,  98.0000,  28.0000,  40.0000],\n",
       "        [ 40.5000, 131.0000,  27.0000,  48.0000]])\n",
       "xywhn: tensor([[0.8182, 0.5355, 0.1018, 0.2186],\n",
       "        [0.1473, 0.7158, 0.0982, 0.2623]])\n",
       "xyxy: tensor([[211.,  78., 239., 118.],\n",
       "        [ 27., 107.,  54., 155.]])\n",
       "xyxyn: tensor([[0.7673, 0.4262, 0.8691, 0.6448],\n",
       "        [0.0982, 0.5847, 0.1964, 0.8470]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Gun', 1: 'Knife'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "boxes: tensor([], size=(0, 6))\n",
       "cls: tensor([])\n",
       "conf: tensor([])\n",
       "data: tensor([], size=(0, 6))\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (159, 318)\n",
       "shape: torch.Size([0, 6])\n",
       "xywh: tensor([], size=(0, 4))\n",
       "xywhn: tensor([], size=(0, 4))\n",
       "xyxy: tensor([], size=(0, 4))\n",
       "xyxyn: tensor([], size=(0, 4))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Keypoints object with attributes:\n",
       "\n",
       "conf: None\n",
       "data: tensor([], size=(1, 0, 51))\n",
       "has_visible: False\n",
       "orig_shape: (159, 318)\n",
       "shape: torch.Size([1, 0, 51])\n",
       "xy: tensor([], size=(1, 0, 2))\n",
       "xyn: tensor([], size=(1, 0, 2))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n",
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ultralytics.engine.results.Boxes object with attributes:\n",
       " \n",
       " boxes: tensor([[  0.0000,  15.2731, 104.5066, 139.5477,   0.5677,   0.0000]])\n",
       " cls: tensor([0.])\n",
       " conf: tensor([0.5677])\n",
       " data: tensor([[  0.0000,  15.2731, 104.5066, 139.5477,   0.5677,   0.0000]])\n",
       " id: None\n",
       " is_track: False\n",
       " orig_shape: (225, 225)\n",
       " shape: torch.Size([1, 6])\n",
       " xywh: tensor([[ 52.2533,  77.4104, 104.5066, 124.2746]])\n",
       " xywhn: tensor([[0.2322, 0.3440, 0.4645, 0.5523]])\n",
       " xyxy: tensor([[  0.0000,  15.2731, 104.5066, 139.5477]])\n",
       " xyxyn: tensor([[0.0000, 0.0679, 0.4645, 0.6202]]),\n",
       " ultralytics.engine.results.Boxes object with attributes:\n",
       " \n",
       " boxes: tensor([[102.5732,  17.3269, 219.4575, 137.6605,   0.5125,   0.0000]])\n",
       " cls: tensor([0.])\n",
       " conf: tensor([0.5125])\n",
       " data: tensor([[102.5732,  17.3269, 219.4575, 137.6605,   0.5125,   0.0000]])\n",
       " id: None\n",
       " is_track: False\n",
       " orig_shape: (225, 225)\n",
       " shape: torch.Size([1, 6])\n",
       " xywh: tensor([[161.0153,  77.4937, 116.8842, 120.3336]])\n",
       " xywhn: tensor([[0.7156, 0.3444, 0.5195, 0.5348]])\n",
       " xyxy: tensor([[102.5732,  17.3269, 219.4575, 137.6605]])\n",
       " xyxyn: tensor([[0.4559, 0.0770, 0.9754, 0.6118]]),\n",
       " ultralytics.engine.results.Boxes object with attributes:\n",
       " \n",
       " boxes: tensor([[102.5732,  17.3269, 219.4575, 137.6605,   0.5125,   0.0000]])\n",
       " cls: tensor([0.])\n",
       " conf: tensor([0.5125])\n",
       " data: tensor([[102.5732,  17.3269, 219.4575, 137.6605,   0.5125,   0.0000]])\n",
       " id: None\n",
       " is_track: False\n",
       " orig_shape: (225, 225)\n",
       " shape: torch.Size([1, 6])\n",
       " xywh: tensor([[161.0153,  77.4937, 116.8842, 120.3336]])\n",
       " xywhn: tensor([[0.7156, 0.3444, 0.5195, 0.5348]])\n",
       " xyxy: tensor([[102.5732,  17.3269, 219.4575, 137.6605]])\n",
       " xyxyn: tensor([[0.4559, 0.0770, 0.9754, 0.6118]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].boxes[0], outputs[0].boxes[1], outputs[0].boxes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].boxes.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[500.55377197265625,\n",
       "  179.87948608398438,\n",
       "  164.24246215820312,\n",
       "  165.65061950683594],\n",
       " [360.8485107421875, 146.62167358398438, 121.546875, 178.2393035888672],\n",
       " [702.9388427734375, 53.9835090637207, 133.1436767578125, 107.9670181274414],\n",
       " [153.8926544189453,\n",
       "  124.35717010498047,\n",
       "  128.75613403320312,\n",
       "  199.0615997314453],\n",
       " [698.9496459960938, 208.50839233398438, 111.9210205078125, 144.3022918701172]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].boxes.xywh.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "boxes: tensor([[376.0414, 282.7526, 916.1423, 649.9327,   0.9333,   0.0000]])\n",
       "cls: tensor([0.])\n",
       "conf: tensor([0.9333])\n",
       "data: tensor([[376.0414, 282.7526, 916.1423, 649.9327,   0.9333,   0.0000]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (740, 1216)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[646.0919, 466.3427, 540.1010, 367.1801]])\n",
       "xywhn: tensor([[0.5313, 0.6302, 0.4442, 0.4962]])\n",
       "xyxy: tensor([[376.0414, 282.7526, 916.1423, 649.9327]])\n",
       "xyxyn: tensor([[0.3092, 0.3821, 0.7534, 0.8783]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bbox': [0.5313255190849304,\n",
      "           0.630192756652832,\n",
      "           0.444161981344223,\n",
      "           0.49618929624557495],\n",
      "  'bbox_std': [646.0918579101562,\n",
      "               466.3426513671875,\n",
      "               540.1009521484375,\n",
      "               367.1800842285156],\n",
      "  'confidence': 0.9333458542823792,\n",
      "  'label': 'Gun',\n",
      "  'orig_shape': (740, 1216)},\n",
      " {'bbox': [0.267814576625824,\n",
      "           0.5433048009872437,\n",
      "           0.09092368930578232,\n",
      "           0.708624541759491],\n",
      "  'bbox_std': [325.66253662109375,\n",
      "               402.0455627441406,\n",
      "               110.56320190429688,\n",
      "               524.3821411132812],\n",
      "  'confidence': 0.7023518085479736,\n",
      "  'label': 'Gun',\n",
      "  'orig_shape': (740, 1216)},\n",
      " {'bbox': [0.5150426626205444,\n",
      "           0.24983258545398712,\n",
      "           0.3836894631385803,\n",
      "           0.28943443298339844],\n",
      "  'bbox_std': [626.2918701171875,\n",
      "               184.87611389160156,\n",
      "               466.5663757324219,\n",
      "               214.18148803710938],\n",
      "  'confidence': 0.5849758982658386,\n",
      "  'label': 'Gun',\n",
      "  'orig_shape': (740, 1216)},\n",
      " {'bbox': [0.824057936668396,\n",
      "           0.51497483253479,\n",
      "           0.15205764770507812,\n",
      "           0.8436135649681091],\n",
      "  'bbox_std': [1002.054443359375,\n",
      "               381.08135986328125,\n",
      "               184.902099609375,\n",
      "               624.2740478515625],\n",
      "  'confidence': 0.48277047276496887,\n",
      "  'label': 'Gun',\n",
      "  'orig_shape': (740, 1216)},\n",
      " {'bbox': [0.18045872449874878,\n",
      "           0.5444341897964478,\n",
      "           0.10244493931531906,\n",
      "           0.6700120568275452],\n",
      "  'bbox_std': [219.43780517578125,\n",
      "               402.88128662109375,\n",
      "               124.57304382324219,\n",
      "               495.8089294433594],\n",
      "  'confidence': 0.47320428490638733,\n",
      "  'label': 'Knife',\n",
      "  'orig_shape': (740, 1216)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "for i in range(len(outputs[0].boxes)):\n",
    "    results.append({\n",
    "        'confidence': outputs[0].boxes[i].conf[0].item(),\n",
    "        'label': outputs[0].names[outputs[0].boxes[i].cls.item()],\n",
    "        'bbox': outputs[0].boxes[i].xywhn[0].tolist(),\n",
    "        'bbox_std': outputs[0].boxes[i].xywh[0].tolist(),\n",
    "        'orig_shape': outputs[0].orig_shape\n",
    "    })\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (1/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 48.6ms\n",
      "video 1/1 (2/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 3 Knifes, 28.9ms\n",
      "video 1/1 (3/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.2ms\n",
      "video 1/1 (4/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 29.0ms\n",
      "video 1/1 (5/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.4ms\n",
      "video 1/1 (6/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.6ms\n",
      "video 1/1 (7/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.2ms\n",
      "video 1/1 (8/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 22.6ms\n",
      "video 1/1 (9/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 23.6ms\n",
      "video 1/1 (10/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 22.8ms\n",
      "video 1/1 (11/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 3 Knifes, 24.1ms\n",
      "video 1/1 (12/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 26.5ms\n",
      "video 1/1 (13/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 26.2ms\n",
      "video 1/1 (14/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 24.2ms\n",
      "video 1/1 (15/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 25.0ms\n",
      "video 1/1 (16/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 24.5ms\n",
      "video 1/1 (17/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 24.8ms\n",
      "video 1/1 (18/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 29.4ms\n",
      "video 1/1 (19/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 28.3ms\n",
      "video 1/1 (20/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 24.5ms\n",
      "video 1/1 (21/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 21.5ms\n",
      "video 1/1 (22/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 23.4ms\n",
      "video 1/1 (23/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 24.3ms\n",
      "video 1/1 (24/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 39.2ms\n",
      "video 1/1 (25/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 74.1ms\n",
      "video 1/1 (26/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.6ms\n",
      "video 1/1 (27/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.7ms\n",
      "video 1/1 (28/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 23.5ms\n",
      "video 1/1 (29/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 25.1ms\n",
      "video 1/1 (30/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 22.5ms\n",
      "video 1/1 (31/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 24.4ms\n",
      "video 1/1 (32/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 20.9ms\n",
      "video 1/1 (33/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 22.4ms\n",
      "video 1/1 (34/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 21.5ms\n",
      "video 1/1 (35/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 22.3ms\n",
      "video 1/1 (36/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 24.0ms\n",
      "video 1/1 (37/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 22.9ms\n",
      "video 1/1 (38/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 21.9ms\n",
      "video 1/1 (39/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 21.9ms\n",
      "video 1/1 (40/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 21.2ms\n",
      "video 1/1 (41/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.9ms\n",
      "video 1/1 (42/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.4ms\n",
      "video 1/1 (43/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 27.1ms\n",
      "video 1/1 (44/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.4ms\n",
      "video 1/1 (45/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.3ms\n",
      "video 1/1 (46/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.8ms\n",
      "video 1/1 (47/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 27.0ms\n",
      "video 1/1 (48/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 26.3ms\n",
      "video 1/1 (49/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.1ms\n",
      "video 1/1 (50/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 3 Knifes, 23.7ms\n",
      "video 1/1 (51/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 25.5ms\n",
      "video 1/1 (52/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.9ms\n",
      "video 1/1 (53/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.6ms\n",
      "video 1/1 (54/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 24.6ms\n",
      "video 1/1 (55/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 1 Knife, 23.6ms\n",
      "video 1/1 (56/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 25.9ms\n",
      "video 1/1 (57/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Guns, 1 Knife, 25.2ms\n",
      "video 1/1 (58/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Guns, 1 Knife, 24.9ms\n",
      "video 1/1 (59/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 22.7ms\n",
      "video 1/1 (60/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 28.9ms\n",
      "video 1/1 (61/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 24.5ms\n",
      "video 1/1 (62/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 23.9ms\n",
      "video 1/1 (63/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.9ms\n",
      "video 1/1 (64/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 25.7ms\n",
      "video 1/1 (65/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 26.4ms\n",
      "video 1/1 (66/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 29.5ms\n",
      "video 1/1 (67/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.0ms\n",
      "video 1/1 (68/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.0ms\n",
      "video 1/1 (69/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.2ms\n",
      "video 1/1 (70/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 21.9ms\n",
      "video 1/1 (71/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 23.1ms\n",
      "video 1/1 (72/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.1ms\n",
      "video 1/1 (73/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 23.7ms\n",
      "video 1/1 (74/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Knife, 26.3ms\n",
      "video 1/1 (75/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.7ms\n",
      "video 1/1 (76/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 25.1ms\n",
      "video 1/1 (77/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 23.3ms\n",
      "video 1/1 (78/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.3ms\n",
      "video 1/1 (79/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.4ms\n",
      "video 1/1 (80/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.5ms\n",
      "video 1/1 (81/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.7ms\n",
      "video 1/1 (82/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 22.2ms\n",
      "video 1/1 (83/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 1 Gun, 2 Knifes, 21.7ms\n",
      "video 1/1 (84/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 48.9ms\n",
      "video 1/1 (85/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 24.5ms\n",
      "video 1/1 (86/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 194.2ms\n",
      "video 1/1 (87/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 185.6ms\n",
      "video 1/1 (88/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 114.9ms\n",
      "video 1/1 (89/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 39.1ms\n",
      "video 1/1 (90/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 22.7ms\n",
      "video 1/1 (91/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 2 Knifes, 35.9ms\n",
      "video 1/1 (92/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 3 Knifes, 39.3ms\n",
      "video 1/1 (93/245) /Users/omkar/projects/rajpol/Backend/models/samples/gun_video_ultra_short.mp4: 256x416 3 Knifes, 40.0ms\n",
      "Speed: 0.9ms preprocess, 30.6ms inference, 0.4ms postprocess per image at shape (1, 3, 256, 416)\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/predict10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# cap = cv2.VideoCapture('rtmp://localhost/mystream')\n",
    "import time\n",
    "\n",
    "NUM_FRAMES = 30  # Number of frames per video clip (must match the training configuration)\n",
    "frame_buffer = []\n",
    "\n",
    "garbage_last_time = 0\n",
    "assault_last_time = 0\n",
    "videoPath = 'samples/gun_video_ultra_short.mp4'\n",
    "# videoPath = 'samples/gun_video_short.mp4'\n",
    "results = weaponsYoloModel.predict(source=videoPath, save=True)\n",
    "counter = 0\n",
    "\n",
    "# cap = cv2.VideoCapture(videoPath)\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     if counter % 50 == 0:\n",
    "#         results = weaponsYoloModel.predict(source=frame, save=True)\n",
    "#         # if list(results[0].boxes.cls).count(0) > 0:\n",
    "#         #     img_with_boxes = results[0].plot()\n",
    "#         #     cv2.imwrite(f'temp-results/weapons/weapon_f{counter}', img_with_boxes)\n",
    "#     counter +=1\n",
    "# cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fight model loaded\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "fightModel = load_model(\"models/weights/fights/fights2vgg2cat2.keras\")\n",
    "print(\"Fight model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples/knife_gun.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m fightModel\u001b[38;5;241m.\u001b[39mpredict(frame)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "frame = cv2.imread(\"samples/knife_gun.jpg\")\n",
    "fightModel.predict(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     if (time.time()-assault_last_time) > 600:\n",
    "#         frame2 = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "#         frame_buffer.append(frame2)\n",
    "\n",
    "#         if len(frame_buffer) > NUM_FRAMES:\n",
    "#             frame_buffer.pop(0)\n",
    "#         if counter % 150 == 0:\n",
    "#         # Perform prediction when the buffer is full\n",
    "#             if len(frame_buffer) == NUM_FRAMES:\n",
    "#                 input_video_clip = np.array(frame_buffer)\n",
    "#                 input_video_clip = np.expand_dims(input_video_clip, axis=0)\n",
    "\n",
    "#                 # Perform predictions\n",
    "#                 predictions = fightModel.predict(input_video_clip)\n",
    "\n",
    "#                 # Get the class with the highest probability as the predicted class\n",
    "#                 predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "#                 # Display the result on the frame\n",
    "#                 print(f\"Predicted Class: {predicted_class[0]}\")\n",
    "#                 if predicted_class[0] == 1:\n",
    "#                     N = 8\n",
    "#                     cv2.imwrite('/home/azureuser/sih/garbage/results/test.jpg', frame)\n",
    "#                     id = ''.join(random.choices(string.ascii_uppercase +\n",
    "#                                             string.digits, k=N))\n",
    "#                     path = \"/home/azureuser/sih/garbage/results/test.jpg\"\n",
    "#                     # folderpath = results[0].save_dir + '/'\n",
    "\n",
    "#                     key_name = f'{id}.jpg'\n",
    "#                     bucket.upload_file(path, key_name)\n",
    "#                     to_insert = {\n",
    "#                     'id':id,\n",
    "#                     'image':f'https://railrakshak.s3.amazonaws.com/{key_name}',\n",
    "#                     'title':\"Violence Detected\",\n",
    "#                     'description':\"Violence Detected via CCTV\",\n",
    "#                     'type':\"Anomaly\",\n",
    "#                     'station_name':\"Andheri\",\n",
    "#                     'location':\"Platform 5\",\n",
    "#                     'source':\"CCTV\",\n",
    "#                     'status':\"Pending\",\n",
    "#                     'created_at': datetime.now(),\n",
    "\n",
    "#                 }\n",
    "#                     incidents.insert_one(to_insert)\n",
    "\n",
    "#                     assault_last_time = time.time()\n",
    "#     counter +=1\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, numpy.int64, 1, int)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "num = np.int64(1)\n",
    "num, num.item(), type(num), int(num), type(int(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9985997, 0.9985997, numpy.float64, 0.9985997, float)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.float64(0.9985997)\n",
    "f, f.item(), type(f), float(f), type(float(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "determined",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
